{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVNX715-kj0z"
   },
   "source": [
    "## Part 1: Multinomial Naive Bayes- (multi-class text classification) on 20 news (DS1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-2-06v_kgWS",
    "outputId": "55c44fa5-9261-4a92-e432-194f3a35d7d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/muchderek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muchderek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace #for debugging\n",
    "import string\n",
    "np.random.seed(1234)\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#=======clean data=======\n",
    "#remove difference between word with Capital first letter  and apple & appleS?\n",
    "def remove_bad_characters(data):\n",
    "  tokens = word_tokenize(data)\n",
    "  # convert to lower case\n",
    "  tokens = [w.lower() for w in tokens]\n",
    "  # remove punctuation from each word\n",
    "  import string\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  stripped = [w.translate(table) for w in tokens]\n",
    "  # remove remaining tokens that are not alphabetic\n",
    "  words = [word for word in stripped if word.isalpha()]\n",
    "  # filter out stop words\n",
    "  from nltk.corpus import stopwords\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [w for w in words if not w in stop_words]\n",
    "  sent = ' '.join(words)\n",
    "  return sent\n",
    "\n",
    "def num_doc_class(target):\n",
    "  import collections\n",
    "  counter=collections.Counter(target)\n",
    "  print(\"Number of document per categories: {}\".format(counter.values()))\n",
    "  print(\"Categorie ID associated with previous numbers: {}\".format(counter.keys()))\n",
    "  counter_n=counter.values()\n",
    "\n",
    "  for i in counter:\n",
    "    counter[i]/=len(train.target)\n",
    "  print(\"Probability of each class:\")\n",
    "  print(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in counter.items()))\n",
    "    \n",
    "  return counter,counter_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2sAS7nylWnf"
   },
   "source": [
    "#20 news group dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvofE8hNH5xJ",
    "outputId": "600bb991-9a47-4b47-c047-0a5b2d4ec9e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document per categories: dict_values([480, 599, 584])\n",
      "Categorie ID associated with previous numbers: dict_keys([0, 2, 1])\n",
      "Probability of each class:\n",
      "0: 0.28863499699338546\n",
      "2: 0.36019242333132895\n",
      "1: 0.35117257967528565\n",
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "We have 3 unique classes\n",
      "We have 1663 training samples\n",
      "We have 1106 test samples\n",
      "Here are the 20 different classes ['alt.atheism', 'soc.religion.christian', 'comp.graphics']\n",
      "and their target number are [0 2 2 ... 1 0 2]\n",
      "The distribution of doc in each class: Counter({2: 0.36019242333132895, 1: 0.35117257967528565, 0: 0.28863499699338546})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##For 20news dataset\n",
    "#======import dataset 20newsgroups=======\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_newgroup=fetch_20newsgroups()\n",
    "\n",
    "#=======load data ======= \n",
    "#categories=twenty_newgroup.target_names --> for all classes\n",
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics'] #to test whithout long execution times\n",
    "train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories) # F-score will be lower because it is more realistic\n",
    "test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "\n",
    "\n",
    "#=======clean data ======= \n",
    "for i in range(len(train.data)): #for every doc \n",
    "    train.data[i]=remove_bad_characters(train.data[i])\n",
    "for i in range(len(test.data)):\n",
    "    test.data[i]=remove_bad_characters(test.data[i])\n",
    "  \n",
    "#=======count data frequency: word apearance in each doc======= #X_tfidf we can try with this later to see diff in accuracy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english') #counts\n",
    "\n",
    "\n",
    "X_train_count = count_vect.fit_transform(train.data) ## count of words in each email/doc for all classes\n",
    "Unique_words_train= count_vect.get_feature_names()#reduces the text into a set of features\n",
    "X_test_count = count_vect.transform(test.data)\n",
    "\n",
    "\n",
    "\n",
    "#=======count data frequency:distriution of doc per class======= \n",
    "counter,counter_n=num_doc_class(train.target) \n",
    "\n",
    "y = train.target\n",
    "\n",
    "#========Probability of each word per class=======\n",
    "#Matrix_df=pd.DataFrame.sparse.from_spmatrix(X_train_count)\n",
    "#testMatrix_df = pd.DataFrame.sparse.from_spmatrix(X_test_count)\n",
    "#print(Matrix_df.iloc[:584,:]) #frequency of each word for each doc of class [1] \n",
    "\n",
    "#x_train = Matrix_df.to_numpy() ################# <===== THIS IS X_TRAIN BUT IN NUMPY PLS USE <=============================== ###########\n",
    "#x_test = testMatrix_df.to_numpy()[:,np.isin(Unique_words_test, Unique_words_train)] #only keep columns of words contained in train set\n",
    "#print(numMat[:584,:])\n",
    "\n",
    "#=======data vizualization=======\n",
    "print(twenty_newgroup.keys())\n",
    "print(\"We have {} unique classes\".format(len(categories)))\n",
    "print(\"We have {} training samples\".format(len(train.data))) # number of text sentences for training\n",
    "print(\"We have {} test samples\".format(len(test.data))) # number of text sentences for testing\n",
    "print(\"Here are the 20 different classes {}\".format(categories))\n",
    "print(\"and their target number are {}\".format(train.target))# the classes\n",
    "#print(\"All words: {}\".format(Unique_words_train)) #all the different words: numbers, signs and than alphabetic order \n",
    "print(\"The distribution of doc in each class: {}\".format(counter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQdHmToYjofe"
   },
   "source": [
    " Implement multinomialnaive Bayes model as a Python class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4vChX9zNeNT"
   },
   "source": [
    "\n",
    "a prior for the probability of each class by counting relative frequency of class labels in the dataset.\n",
    "for multi-class classification, wich $C$ classes, this is a categorical distribution $\\mathrm{Categorical}(y=c_0; \\pi) = \\prod_c \\pi_c^{\\mathbb{I}(y=c)} = \\pi_{c_0}$. MLE is given by $\\pi_c = \\frac{N(y=c)}{N}$.\n",
    "a probability for the value of each feature $x_d$, conditioned on the class label $y$. The type of this probability distribution depends on the feature type. \n",
    "Once we learn both the class prior $p(y)$ and class conditionals $p(x_d|y)$ from the data, we can use the Bayes rule to calculate posterior class probability $p(y| x) \\propto p(y) p(x| y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9G0TWdNNM9BU"
   },
   "outputs": [],
   "source": [
    "def logsumexp(Z):                                                # dimension C x N\n",
    "      Zmax = np.max(Z)                              # max over C\n",
    "      log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax)))\n",
    "      return log_sum_exp\n",
    "    \n",
    "class MultNaiveBayes:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y, alpha, beta): # x is the document matrix, y is train.target\n",
    "      N, D = x.shape # N = number of documents in total, D = words d (columns)\n",
    "      self.C = np.max(y) + 1 # C is number of categories\n",
    "      # one parameter for each feature conditioned on each class\n",
    "      self.pi = np.zeros(self.C) # store prior values for each category\n",
    "      Nc = np.zeros(self.C) # store number of instances in class c\n",
    "      self.coeffs = np.zeros((self.C,D))\n",
    "      \n",
    "      # get the priors\n",
    "      #import collections\n",
    "      #DocsPerCat=collections.Counter(y)\n",
    "      #print(\"Number of document per categories: {}\".format(counter.values()))\n",
    "      #print(\"Categorie ID associated with previous numbers: {}\".format(counter.keys()))\n",
    "      #for i in DocsPerCat:\n",
    "       # self.pi[i]=(DocsPerCat[i]+alpha)/(len(y)+alpha+beta)\n",
    "      \n",
    "      # get likelihood coefficients\n",
    "      for c in range(self.C): \n",
    "        self.pi[c]=(x[y==c].shape[0]+alpha)/(N+alpha+beta)\n",
    "        wtot = np.sum(np.sum(x[y == c,:], axis = 0)) #get total label count of this category\n",
    "        SumD = np.sum(x[y == c,:], axis = 0) #return a 1d array of sum of the elements of each column \n",
    "        self.coeffs[c,:] = (SumD+1)/(wtot+D) #get likelihood\n",
    "\n",
    "      return self\n",
    "\n",
    "    def predict(self, xt): #xt contains word d columns that exist in train set\n",
    "        Nt, D = xt.shape #Nt is number of rows, ie number of test cases to predict, #D is number of words d (columns)\n",
    "        y_prob = np.zeros(Nt)\n",
    "        posterior = np.zeros((Nt, self.C)) #store posterior values for each category\n",
    "        log_likelihood = np.zeros(self.C)\n",
    "\n",
    "        log_prior = np.log(self.pi)\n",
    "        for n in range(Nt):\n",
    "            for c in range(self.C):\n",
    "                #xtemp = xt[n,:]\n",
    "                xtemp = xt[n].T\n",
    "                x_=np.array(np.argwhere(xtemp > 0))\n",
    "                x__=np.delete(x_,1,1)\n",
    "                #log_likelihood[c] = np.sum(np.log(self.coeffs[c,np.isin(wordsTrain, wordsTest[xtemp[xtemp>0]])]))\n",
    "                log_likelihood[c] = np.sum(np.log(self.coeffs[c,x__]))\n",
    "                #log_likelihood = np.sum(log_likelihood, axis = 1) #perform sum across columns\n",
    "            log_posterior = log_prior + log_likelihood\n",
    "            posterior[n,:] = np.exp(log_posterior - logsumexp(log_posterior)) # normalization...; dim N x C\n",
    "        return np.argmax(posterior,1) #or return y_pred directly by getting the argmax.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1AvP4MBgQ9D"
   },
   "source": [
    "Experiment:\n",
    "Apply model to classify documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "0agRlrEYgQE_",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8426763110307414\n"
     ]
    }
   ],
   "source": [
    "#y_train/test is train/test.target\n",
    "import timeit\n",
    "#X_train = X_train_count[0:int(X_train_count.shape[0] * train_size[each])]\n",
    "y_train_less = train.target[0:int(train.target.shape[0]*1)] # try portions of data\n",
    "#y_test = test.target[0:int(test.target.shape[0]*0.2)]\n",
    "X_train_count_less = X_train_count[0:int(X_train_count.shape[0]*1)]\n",
    "model = MultNaiveBayes()\n",
    "t1 = timeit.default_timer()\n",
    "model.fit(X_train_count_less, y_train_less, 1, 1)\n",
    "train_time = timeit.default_timer() - t1\n",
    "y_pred = model.predict(X_test_count)\n",
    "accuracy = np.sum(y_pred == test.target)/y_pred.shape[0]\n",
    "print(f'test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663\n",
      "(1663, 21083)\n",
      "(1663,)\n",
      "(332,)\n",
      "(1663, 21083)\n"
     ]
    }
   ],
   "source": [
    "print(train.target.shape[0])\n",
    "#1663 rows\n",
    "#ok = np.delete(X_train_count, np.s_[1:100], axis = 0)\n",
    "print(X_train_count.shape)\n",
    "print(train.target.shape)\n",
    "print(y_train.shape)\n",
    "train.target.shape\n",
    "print(X_train_count[0:int(X_train_count.shape[0]*1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2294095510005718\n"
     ]
    }
   ],
   "source": [
    "print(train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on DS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for model multinomial: 64.12371134020619\n",
      "14.007876427000156\n",
      "[0.64078283 0.63699495 0.62058081 0.66161616 0.63044852]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "\n",
    "def Logis_Regression(iters,norm,X_train,y_train,X_test,y_test,c):\n",
    "  \n",
    "    for this_max_iter in iters:\n",
    "\n",
    "        lr=LogisticRegression(C=c,solver='saga',multi_class='multinomial',penalty=norm,max_iter=this_max_iter,random_state=0)\n",
    "        t1 = timeit.default_timer()\n",
    "        lr.fit(X_train,y_train)\n",
    "        train_time = timeit.default_timer() - t1\n",
    "        y_pred=lr.predict(X_test)\n",
    "\n",
    "        #accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
    "        accuracy=accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy,train_time\n",
    "\n",
    "#=======load data ======= \n",
    "X_data, y_data = fetch_20newsgroups_vectorized(remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_data,y_data,test_size=0.3)\n",
    "\n",
    "#Import data set\n",
    "#Hyperparameters\n",
    "iters=[5.78]\n",
    "norm='l1'\n",
    "c=9.8\n",
    "#calling function\n",
    "Accuracy_regression_DS1,train_time=Logis_Regression(iters,norm,X_train,y_train,X_test,y_test,c)\n",
    "print('Test accuracy for model {}: {}'.format('multinomial', Accuracy_regression_DS1*100))\n",
    "print(train_time)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# kfold = KFold(n_splits=5, shuffle=True)\n",
    "# lr=LogisticRegression(C=c,solver='saga',multi_class='multinomial',penalty=norm,max_iter=20,random_state=0)\n",
    "# results = cross_val_score(lr, X_train, y_train, cv=kfold)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muchderek/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/muchderek/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/muchderek/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/muchderek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'max_iter': 5.775510204081633, 'C': 9.806122448979592}\n"
     ]
    }
   ],
   "source": [
    "##TUNNING HYPERPARAMETERS \n",
    "C = np.linspace(0.5,10)\n",
    "penalty = ['l1','l2']\n",
    "iter = np.linspace(1,10)\n",
    "hyperparameters = dict(C=C, penalty=penalty, max_iter=iter)\n",
    "lr_tune = LogisticRegression()\n",
    "rsearch = RandomizedSearchCV(lr_tune, hyperparameters, n_iter=10)\n",
    "rsearch.fit(X_train,y_train)\n",
    "print(rsearch.best_params_)\n",
    "\n",
    "# #grid search\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# lrGrid = LogisticRegression()\n",
    "# C = np.linspace(0.5,10)\n",
    "# penalty = ['l1','l2']\n",
    "# iter = np.linspace(1,10)\n",
    "# hyperparameters = dict(C=C, penalty=penalty, max_iter=iter)\n",
    "\n",
    "# gridlr = GridSearchCV(lrGrid, hyperparameters, cv=5, verbose = 0)\n",
    "# best_model = gridlr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
